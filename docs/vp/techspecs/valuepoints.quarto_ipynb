{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: Using SQL on FHIR to Generate ValuePoints from Bulk FHIR Exports\n",
        "---"
      ],
      "id": "7fbb06c6"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This section provides an example of how to work with anonymized data to generate value points.\n",
        "\n",
        "\n",
        "## Step 1: Creating View Definitions for FHIR Resources\n",
        "\n",
        "The first step involves creating view definitions for various FHIR resources. These view definitions are essential for transforming the hierarchical structure of FHIR resources into flattened relational tables, enabling easier querying and analysis using SQL. Below is an example of a view definition for the `Patient` resource:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"name\": \"mnch_patient\",\n",
        "  \"title\": \"MNCH Patient View\",\n",
        "  \"version\": \"0.1.0\",\n",
        "  \"url\": \"https://momcare.cot.pharmaccess.org/fhir/ViewDefinition/patient\",\n",
        "  \"meta\": {\n",
        "    \"profile\": [\n",
        "      \"http://hl7.org/fhir/uv/sql-on-fhir/StructureDefinition/ShareableViewDefinition\",\n",
        "      \"http://hl7.org/fhir/uv/sql-on-fhir/StructureDefinition/TabularViewDefinition\"\n",
        "    ]\n",
        "  },\n",
        "  \"status\": \"draft\",\n",
        "  \"resource\": \"Patient\",\n",
        "  \"fhirVersion\": [\n",
        "    \"4.0.1\"\n",
        "  ],\n",
        "  \"select\": [\n",
        "    {\n",
        "      \"column\": [\n",
        "        {\n",
        "          \"name\": \"birth_date\",\n",
        "          \"path\": \"birthDate\",\n",
        "          \"type\": \"datetime\",\n",
        "          \"collection\": false\n",
        "        },\n",
        "        {\n",
        "          \"name\": \"patient_id\",\n",
        "          \"path\": \"id\",\n",
        "          \"type\": \"ID\",\n",
        "          \"collection\": false\n",
        "        }\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"forEachOrNull\": \"identifier\",\n",
        "      \"column\": [\n",
        "        {\n",
        "          \"name\": \"identifier_code\",\n",
        "          \"path\": \"type.coding.code\",\n",
        "          \"type\": \"code\",\n",
        "          \"collection\": false\n",
        "        },\n",
        "        {\n",
        "          \"name\": \"identifier_system_id\",\n",
        "          \"path\": \"value\",\n",
        "          \"type\": \"string\",\n",
        "          \"collection\": false\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "## Step 2: Extracting Tabular Results with Pathling\n",
        "\n",
        "In this step, we use Pathling, an open-source solution built on Apache Spark, to extract tabular results from bulk FHIR exports using the view definitions created in Step 1.\n",
        "\n",
        "The output of Pathling is a Spark DataFrame, which can be easily loaded into a database for further analysis. For this demonstration, we will use DuckDB to store and query the extracted data."
      ],
      "id": "8ccc068f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pathling import DataSource, PathlingContext\n",
        "from pyspark.sql import DataFrame, SparkSession, Window\n",
        "\n",
        "# create a spark session\n",
        "spark = (\n",
        "            SparkSession.builder.config(\n",
        "                \"spark.jars.packages\",\n",
        "                \"au.csiro.pathling:library-runtime:8.0.0-SNAPSHOT,\"\n",
        "                \"io.delta:delta-spark_2.12:3.2.0,\"\n",
        "                \"org.apache.hadoop:hadoop-aws:3.3.4\",\n",
        "            )\n",
        "            .config(\"spark.jars.repositories\", \"https://oss.sonatype.org/content/repositories/snapshots/\")\n",
        "            .config(\n",
        "                \"spark.sql.extensions\",\n",
        "                \"io.delta.sql.DeltaSparkSessionExtension\",\n",
        "            )\n",
        "            .config(\n",
        "                \"spark.sql.catalog.spark_catalog\",\n",
        "                \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
        "            )\n",
        "            .config(\"spark.driver.memory\", \"5g\")\n",
        "            .getOrCreate()\n",
        "        )\n",
        "pathling_context = PathlingContext.create(spark=spark)"
      ],
      "id": "52b05627",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 preview the flattened patient data\n",
        "\n",
        "in this example we will be assuming the bulk exports are in ndjson format and we will be using the `patient` resource type."
      ],
      "id": "fd844517"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import builtins\n",
        "fhir_data = pathling_context.read.ndjson('./bulk_fhir')\n",
        "# use the view definition to flatten the data\n",
        "with builtins.open('./views/Patient.ViewDefinition.json') as f:\n",
        "    patient_view = fhir_data.view(resource='Patient', json=f.read())\n",
        "    patient_view.show(5)\n"
      ],
      "id": "fdce6f10",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Defining Resources and Creating Patient Timeline\n",
        "\n",
        "In this step, we define all the required FHIR resources, flatten them using the view definitions, and transform the data into a patient timeline. This timeline provides a comprehensive view of patient events, enabling detailed analysis and insights."
      ],
      "id": "dacfcf00"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from helper_functions import helpers\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import DateType, LongType, StringType\n",
        "# define required resources\n",
        "required_resources = [\n",
        "            \"Condition\",\n",
        "            \"Encounter\",\n",
        "            \"MedicationDispense\",\n",
        "            \"Observation\",\n",
        "            \"Patient\",\n",
        "            \"Procedure\",\n",
        "        ]\n",
        "# read bulk data\n",
        "fhir_data = pathling_context.read.ndjson('./bulk_fhir')\n",
        "\n",
        "reference_columns = [\n",
        "            \"visit_provider_id\",\n",
        "            \"patient_id\",\n",
        "            \"encounter_id\",\n",
        "            \"account_id\",\n",
        "        ]\n",
        "\n",
        "views = {}\n",
        "for resource in required_resources:\n",
        "    with builtins.open(f'./views/{resource}.ViewDefinition.json') as f:\n",
        "        resource_view = fhir_data.view(resource=resource, json=f.read())\n",
        "        # this removes backward references i.e paitent/1 -> 1\n",
        "        # this can also be avoided by using correct view definition\n",
        "        cleaned_view = helpers.clean_resource_references(\n",
        "            resource_view, reference_columns\n",
        "        )\n",
        "        views[resource] = cleaned_view\n",
        "        \n",
        "# create a base view to join other tables\n",
        "encounters = views[\"Encounter\"]\n",
        "encounters = encounters.withColumn(\n",
        "    \"visit_type_code\", encounters.visit_type_code.cast(LongType())\n",
        ")\n",
        "encounters = encounters.withColumn(\n",
        "    \"visit_start_date\", encounters.visit_start_date.cast(DateType())\n",
        ")\n",
        "encounters = encounters.withColumn(\n",
        "    \"visit_end_date\", encounters.visit_end_date.cast(DateType())\n",
        ")\n",
        "base = encounters.where(F.col(\"visit_start_date\") > \"1900-01-01\")\n",
        "\n",
        "base.show(5)"
      ],
      "id": "65031dd3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3.2: use helper functions to extract patient timeline"
      ],
      "id": "654858c5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# import importlib\n",
        "# importlib.reload(helpers)\n",
        "\n",
        "procedure = helpers.get_procedure(base, views)\n",
        "condition = helpers.get_diagnosis(base, views)\n",
        "medication = helpers.get_medication(base, views)\n",
        "observation = helpers.get_observation(base, views)\n",
        "\n",
        "timeline = procedure.union(condition).union(medication).union(observation)\n",
        "patient_timeline = timeline.withColumn(\n",
        "            \"value_string\", F.col(\"value_string\").cast(StringType())\n",
        "        )\n",
        "patient_timeline.show(5)"
      ],
      "id": "0b78538e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Extracting Visit Information for Value Points Calculation\n",
        "\n",
        "Once the patient timeline is generated, the next step involves extracting detailed visit information. By aggregating and transforming this data, one can calculate value points, which serve as key indicators for clinical and operational decision-making.\n"
      ],
      "id": "d14a3103"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}